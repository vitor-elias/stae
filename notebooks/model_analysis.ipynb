{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import contextily as cx\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import torch\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "from shapely.geometry import MultiPoint\n",
    "from sklearn.cluster import KMeans\n",
    "from tsmoothie import LowessSmoother, ExponentialSmoother\n",
    "from pyprojroot import here\n",
    "from scipy.spatial import ConvexHull\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import source.nn.models as models\n",
    "import source.utils.utils as utils\n",
    "import source.utils.fault_detection as fd\n",
    "\n",
    "from source.utils.utils import roc_params, compute_auc, get_auc, best_mcc, best_f1score, otsuThresholding\n",
    "from source.utils.utils import synthetic_timeseries\n",
    "from source.utils.utils import plotly_signal\n",
    "\n",
    "from importlib import reload\n",
    "models = reload(models)\n",
    "utils = reload(utils)\n",
    "fd = reload(fd)\n",
    "\n",
    "from pyprojroot import here\n",
    "root_dir = str(here())\n",
    "\n",
    "insar_dir = os.path.expanduser('~/data/raw/')\n",
    "data_path = root_dir + '/data/interim/'\n",
    "dataset_path = root_dir + \"/data/datasets/pixel_detection/\"\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_mse(output,X):\n",
    "    point_mse = torch.nn.MSELoss(reduction='none')\n",
    "    return torch.mean(point_mse(output,X), axis=1)\n",
    "\n",
    "\n",
    "device = 'cuda:2'\n",
    "def train_model(model, X, label, lr, G=None):\n",
    "\n",
    "    rng_seed = 0\n",
    "    torch.manual_seed(rng_seed)\n",
    "    torch.cuda.manual_seed(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    loss_epoch = []\n",
    "    auc_epoch = []\n",
    "    scores_epoch = []\n",
    "\n",
    "    if G is not None:\n",
    "        A = torch.tensor(G.W.toarray()).float()\n",
    "        A = A.to(device)    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    def pixel_mse(output,X):\n",
    "        point_mse = torch.nn.MSELoss(reduction='none')\n",
    "        return torch.mean(point_mse(output,X), axis=1)\n",
    "\n",
    "    model.train()\n",
    "    model.reset_parameters()\n",
    "\n",
    "    # for epoch in range(1, 1+np.max(epochs_list)):\n",
    "    for epoch in range(1,1000):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        if epoch in np.ceil(np.geomspace(1,1000,10)):\n",
    "\n",
    "\n",
    "            loss_epoch.append(loss.item())\n",
    "\n",
    "            scores = pixel_mse(output, X).detach().cpu().numpy()\n",
    "            # scores_epoch.append(scores)\n",
    "\n",
    "            auc = get_auc(scores, label, resolution=101).round(3)\n",
    "            auc_epoch.append(auc)\n",
    "\n",
    "\n",
    "        # if epoch in epochs_list:\n",
    "        #     S_partials.append(S)...\n",
    "\n",
    "    return auc_epoch, loss_epoch\n",
    "\n",
    "def evaluate_model(model, datasets, lr):\n",
    "\n",
    "    auc_epoch_list = []\n",
    "    loss_epoch_list = []\n",
    "\n",
    "    it = 0\n",
    "    for dataset in datasets[:5]:\n",
    "\n",
    "        print(f'Evaluating dataset {it}', flush=True)\n",
    "        it+=1\n",
    "\n",
    "        data = dataset['data']\n",
    "        label = dataset['label'].max(axis=1) #label per pixel\n",
    "        \n",
    "        X = torch.tensor(data).float().to(device)\n",
    "\n",
    "        auc, loss = train_model(model, X, label, lr)\n",
    "        auc_epoch_list.append(auc)\n",
    "        loss_epoch_list.append(loss)\n",
    "\n",
    "        # auc_list.append(get_auc(scores, label).round(3))\n",
    "        # f1_list.append(best_f1score(scores, label).round(3))\n",
    "        # mcc_list.append(best_mcc(scores, label).round(3))\n",
    "\n",
    "    return np.mean(auc_epoch_list, axis=0).round(3), np.mean(loss_epoch_list, axis=0).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(li)[::-1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "datafile = 'pixel_detection/Oslo/training/dataset.pt'\n",
    "datasets = torch.load(dataset_path + datafile)\n",
    "\n",
    "dataset = datasets[0]\n",
    "n_sensors = datasets[0]['data'].shape[0]\n",
    "input_dim = datasets[0]['data'].shape[1]\n",
    "\n",
    "\n",
    "pkl_files = glob.glob(root_dir + '/outputs/HP_training/Geological_anomaly/*.pkl')\n",
    "\n",
    "# Iterate through each file, load the study, and create the model\n",
    "models_dict = {}  # Dictionary to store the models\n",
    "\n",
    "# Collect data for all trials\n",
    "\n",
    "for file in pkl_files:\n",
    "    study = joblib.load(file)\n",
    "    model_name = os.path.basename(file)[3:-4]\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "    trial_values = []\n",
    "    num_params_list = []\n",
    "\n",
    "    for trial in study.trials:\n",
    "        best_params = trial.params\n",
    "        if 'RAE' in model_name:\n",
    "            model_params = {\n",
    "                'n_features': n_sensors,\n",
    "                'latent_dim': best_params['latent_dim'],\n",
    "                'rnn_type': model_name[4:], \n",
    "                'rnn_act': 'relu',\n",
    "                'device': device\n",
    "            }\n",
    "            model_class = getattr(models, 'RAE')\n",
    "            model = model_class(**model_params)\n",
    "            model = model.to(device)\n",
    "\n",
    "        elif model_name == 'GUNet':\n",
    "            model_params = {\n",
    "                'in_channels': input_dim,\n",
    "                'out_channels': input_dim,\n",
    "                'hidden_channels': best_params['hidden_channels'],\n",
    "                'depth': best_params['depth'],\n",
    "                'pool_ratios': best_params['pool_ratios']\n",
    "            }\n",
    "            model_class = getattr(models, model_name)\n",
    "            model = model_class(**model_params)\n",
    "\n",
    "        else:\n",
    "            layer_dims = [input_dim]\n",
    "            current_dim = input_dim * 2\n",
    "            for i in range(best_params['n_layers']):\n",
    "                next_dim = best_params[f'layer_dim_{i}']\n",
    "                layer_dims.append(next_dim)\n",
    "                current_dim = next_dim\n",
    "\n",
    "            model_params = {'layer_dims': layer_dims}\n",
    "            model_class = getattr(models, model_name)\n",
    "            model = model_class(**model_params)\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        trial_values.append(trial.value)\n",
    "        num_params_list.append(num_params)\n",
    "\n",
    "    fig = px.scatter(x=num_params_list, y=trial_values, labels={'x': 'Number of Parameters', 'y': 'Trial Value'}, title='Trial Value vs Number of Parameters')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "datafile = 'pixel_detection/Oslo/training/dataset.pt'\n",
    "datasets = torch.load(dataset_path + datafile)\n",
    "\n",
    "dataset = datasets[0]\n",
    "n_sensors = datasets[0]['data'].shape[0]\n",
    "input_dim = datasets[0]['data'].shape[1]\n",
    "\n",
    "\n",
    "pkl_files = glob.glob(root_dir + '/outputs/HP_training/Geological_anomaly/*.pkl')\n",
    "\n",
    "# Iterate through each file, load the study, and create the model\n",
    "models_dict = {}  # Dictionary to store the models\n",
    "\n",
    "print(f\"{'Model':<15} {'AUC':<10} {'Parameters':<10}\")\n",
    "\n",
    "\n",
    "for file in pkl_files:\n",
    "    study = joblib.load(file)\n",
    "    model_name = os.path.basename(file)[3:-4]\n",
    "\n",
    "    # Get the 10 best trials\n",
    "    best_trials = sorted(study.trials, key=lambda trial: trial.value)[::-1][:10]\n",
    "\n",
    "    for trial in best_trials:\n",
    "        best_params = trial.params\n",
    "        # Initialize the model based on the best parameters\n",
    "        if 'RAE' in model_name:\n",
    "            model_params = {\n",
    "                'n_features': n_sensors,\n",
    "                'latent_dim': best_params['latent_dim'],\n",
    "                'rnn_type': model_name[4:], \n",
    "                'rnn_act': 'relu',\n",
    "                'device': device\n",
    "            }\n",
    "            model_class = getattr(models, 'RAE')\n",
    "            model = model_class(**model_params)\n",
    "            model = model.to(device)\n",
    "\n",
    "        elif model_name == 'GUNet':\n",
    "            model_params = {\n",
    "                'in_channels': input_dim,\n",
    "                'out_channels': input_dim,\n",
    "                'hidden_channels': best_params['hidden_channels'],\n",
    "                'depth': best_params['depth'],\n",
    "                'pool_ratios': best_params['pool_ratios']\n",
    "            }\n",
    "            model_class = getattr(models, model_name)\n",
    "            model = model_class(**model_params)\n",
    "\n",
    "        else:\n",
    "            # For other models\n",
    "            layer_dims = [input_dim]\n",
    "            current_dim = input_dim * 2\n",
    "            for i in range(best_params['n_layers']):\n",
    "                next_dim = best_params[f'layer_dim_{i}']\n",
    "                layer_dims.append(next_dim)\n",
    "                current_dim = next_dim\n",
    "\n",
    "            model_params = {'layer_dims': layer_dims}\n",
    "            model_class = getattr(models, model_name)\n",
    "            model = model_class(**model_params)\n",
    "\n",
    "        models_dict[f\"{model_name}_trial_{trial.number}\"] = model\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"{model_name:<15} {trial.value:<10.3f} {num_params:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "datafile = 'pixel_detection/Oslo/training/dataset.pt'\n",
    "datasets = torch.load(dataset_path + datafile)\n",
    "\n",
    "dataset = datasets[0]\n",
    "n_sensors = datasets[0]['data'].shape[0]\n",
    "input_dim = datasets[0]['data'].shape[1]\n",
    "\n",
    "\n",
    "pkl_files = glob.glob(root_dir + '/outputs/HP_training/Geological_anomaly/*.pkl')\n",
    "\n",
    "# Iterate through each file, load the study, and create the model\n",
    "models_dict = {}  # Dictionary to store the models\n",
    "\n",
    "print(f\"{'Model':<15} {'AUC':<10} {'Parameters':<10}\")\n",
    "\n",
    "\n",
    "for file in pkl_files:\n",
    "\n",
    "    study = joblib.load(file)\n",
    "    model_name = os.path.basename(file)[3:-4]\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    # Initialize the model based on the best parameters\n",
    "    if 'RAE' in model_name:\n",
    "        model_params = {\n",
    "            'n_features': n_sensors,\n",
    "            'latent_dim': best_params['latent_dim'],\n",
    "            'rnn_type': model_name[4:], \n",
    "            'rnn_act': 'relu',\n",
    "            'device': device  # Assuming this is predefined\n",
    "        }\n",
    "        model_class = getattr(models, 'RAE')\n",
    "        model = model_class(**model_params)\n",
    "        model = model.to(device)\n",
    "\n",
    "    elif model_name == 'GUNet':\n",
    "        model_params = {\n",
    "            'in_channels': input_dim,  # Assuming this is predefined\n",
    "            'out_channels': input_dim,\n",
    "            'hidden_channels': best_params['hidden_channels'],\n",
    "            'depth': best_params['depth'],\n",
    "            'pool_ratios': best_params['pool_ratios']\n",
    "        }\n",
    "        model_class = getattr(models, model_name)\n",
    "        model = model_class(**model_params)\n",
    "\n",
    "    else:\n",
    "        # For other models\n",
    "        layer_dims = [input_dim]  # Assuming this is predefined\n",
    "        current_dim = input_dim * 2\n",
    "        for i in range(best_params['n_layers']):\n",
    "            next_dim = best_params[f'layer_dim_{i}']\n",
    "            layer_dims.append(next_dim)\n",
    "            current_dim = next_dim\n",
    "        \n",
    "        model_params = {'layer_dims': layer_dims}\n",
    "        model_class = getattr(models, model_name)\n",
    "        model = model_class(**model_params)\n",
    "\n",
    "    models_dict[model_name] = model\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{model_name:<15} {study.best_value:<10.3f} {num_params:<10}\")\n",
    "    # print(f\"Model: {model_name} - {study.best_value} AUC - {num_params} parameters\\n\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all trials and sort them by value in ascending order (assuming lower is better)\n",
    "best_trials = sorted(study.trials, key=lambda trial: trial.value)[:10]\n",
    "\n",
    "# Extract the corresponding values\n",
    "best_values = [trial.value for trial in best_trials]\n",
    "best_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "datafile = 'pixel_detection/Oslo/training/dataset.pt'\n",
    "datasets = torch.load(dataset_path + datafile)\n",
    "\n",
    "dataset = datasets[0]\n",
    "input_dim = datasets[0]['data'].shape[1]\n",
    "\n",
    "data = dataset['data']\n",
    "label = dataset['label'].max(axis=1) #label per pixel\n",
    "\n",
    "X = torch.tensor(data).float().to( device )\n",
    "\n",
    "\n",
    "# Define the directory containing the Optuna study files\n",
    "pkl_files = glob.glob(root_dir + '/outputs/HP_training/Geological_anomaly/*.pkl')\n",
    "\n",
    "# Iterate through each file, load the study, and create the model\n",
    "models_dict = {}  # Dictionary to store the models\n",
    "\n",
    "for file in pkl_files:\n",
    "    # Load the Optuna study\n",
    "    study = joblib.load(file)\n",
    "    \n",
    "    # Get the best parameters and the model name\n",
    "    best_params = study.best_trial.params\n",
    "    model_name = os.path.basename(file)[3:-4]  # Extract model name from the filename\n",
    "    \n",
    "    # Initialize the model based on the best parameters\n",
    "    if 'RAE' in model_name:\n",
    "        model_params = {\n",
    "            'n_features': 2,\n",
    "            'latent_dim': best_params['latent_dim'],\n",
    "            'rnn_type': model_name[4:], \n",
    "            'rnn_act': 'relu',\n",
    "            'device': device  # Assuming this is predefined\n",
    "        }\n",
    "        model_class = getattr(models, model_name)\n",
    "        model = model_class(**model_params)\n",
    "        model = model.to(device)\n",
    "    \n",
    "    elif model_name == 'GUNet':\n",
    "        model_params = {\n",
    "            'in_channels': input_dim,  # Assuming this is predefined\n",
    "            'out_channels': input_dim,\n",
    "            'hidden_channels': best_params['hidden_channels'],\n",
    "            'depth': best_params['depth'],\n",
    "            'pool_ratios': best_params['pool_ratios']\n",
    "        }\n",
    "        model_class = getattr(models, model_name)\n",
    "        model = model_class(**model_params)\n",
    "    \n",
    "    else:\n",
    "        # For other models\n",
    "        layer_dims = [input_dim]  # Assuming this is predefined\n",
    "        current_dim = input_dim * 2\n",
    "        for i in range(best_params['n_layers']):\n",
    "            next_dim = best_params[f'layer_dim_{i}']\n",
    "            layer_dims.append(next_dim)\n",
    "            current_dim = next_dim\n",
    "        \n",
    "        model_params = {'layer_dims': layer_dims}\n",
    "        model_class = getattr(models, model_name)\n",
    "        model = model_class(**model_params)\n",
    "    \n",
    "    # Store the model in the dictionary\n",
    "    models_dict[model_name] = model\n",
    "\n",
    "# Print the created models\n",
    "for model_name, model in models_dict.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X.clone()\n",
    "\n",
    "X2 = X2.view(-1, X.shape[1], 1)\n",
    "\n",
    "dataset = TensorDataset(X2, X2)  # we want to reconstruct the same input\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# Create an iterator\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Get the first batch\n",
    "batch_X, batch_y = next(data_iter)\n",
    "\n",
    "if model.n_features>1:\n",
    "    batch_X2 = batch_X.T.unsqueeze(0)\n",
    "\n",
    "print(batch_X.shape)\n",
    "print(batch_X2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X.T.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'n_features': 2,\n",
    "                'latent_dim': 4,\n",
    "                'rnn_type': 'LSTM',\n",
    "                'rnn_act': 'relu',\n",
    "                'device': device}\n",
    "batch_size = 512\n",
    "\n",
    "model_class = getattr(models, 'RAE')\n",
    "model = model_class(**model_params)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_params = ['n_features', 'latent_dim', 'rnn_type', 'rnn_act', 'device']\n",
    "model_params = {key: getattr(model, key) for key in relevant_params if hasattr(model, key)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "batch_size = 27\n",
    "seq_len = 10\n",
    "\n",
    "x = torch.tensor([])\n",
    "for i in range(seq_len):\n",
    "    x_i = i*torch.ones([batch_size, n_features])\n",
    "\n",
    "    if x_i.dim() == 1:\n",
    "        x = torch.cat([x, x_i.unsqueeze(0)], axis=1)\n",
    "    else:\n",
    "        x = torch.cat([x, x_i], axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(-1, seq_len, n_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params['n_features'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = joblib.load(root_dir+'/outputs/pixel_detection/HP_training/TR_AE.pkl')\n",
    "datasets = torch.load(dataset_path + 'Oslo/training/dataset.pt')\n",
    "input_dim = datasets[0]['data'].shape[1]\n",
    "\n",
    "dataset = datasets[9]\n",
    "data = dataset['data']\n",
    "label = dataset['label'].max(axis=1) #label per pixel\n",
    "X = torch.tensor(data).float().to(device)\n",
    "\n",
    "px.imshow(dataset['label'], aspect='auto', width=600, title=f'Example: {label.sum():.3g} anomalous nodes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[9]\n",
    "print(dataset['metadata'])\n",
    "data = dataset['data']\n",
    "label = dataset['label'].max(axis=1) #label per pixel\n",
    "X = torch.tensor(data).float().to(device)\n",
    "\n",
    "lr = study.best_params['lr']\n",
    "n_epochs = study.best_params['n_epochs']\n",
    "n_layers = study.best_params['n_layers']\n",
    "layer_dims = [input_dim]\n",
    "for i in range(n_layers):\n",
    "    layer_dims.append(study.best_params[f'layer_dim_{i}'])\n",
    "\n",
    "# dims = [177, 89, 49, 35, 17]\n",
    "# layer_dims = [input_dim, *dims]\n",
    "# lr = 0.000025\t\n",
    "# n_epochs = 261\n",
    "\n",
    "model = models.AE(layer_dims)\n",
    "model = model.to(device)\n",
    "\n",
    "rng_seed = 0\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "model.reset_parameters()\n",
    "\n",
    "output_list = []\n",
    "\n",
    "# for epoch in range(1, 1+np.max(epochs_list)):\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, X)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    output_list.append(output)\n",
    "\n",
    "scores = pixel_mse(output_list[-1], X).detach().cpu().numpy()\n",
    "auc = get_auc(scores, label, resolution=101).round(3)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "df_X = pd.DataFrame(X.detach().cpu().numpy())\n",
    "df_output = pd.DataFrame(output_list[-1].detach().cpu().numpy())\n",
    "\n",
    "# Assign sensor IDs as index\n",
    "df_X.index.name = \"sensor_id\"\n",
    "df_output.index.name = \"sensor_id\"\n",
    "\n",
    "# Melt to long format\n",
    "df_X_long = df_X.reset_index().melt(id_vars=[\"sensor_id\"], var_name=\"timestamp\", value_name=\"X\")\n",
    "df_output_long = df_output.reset_index().melt(id_vars=[\"sensor_id\"], var_name=\"timestamp\", value_name=\"output\")\n",
    "\n",
    "# Merge both DataFrames\n",
    "df_final = pd.merge(df_X_long, df_output_long, on=[\"sensor_id\", \"timestamp\"])\n",
    "\n",
    "# Convert timestamp to integer (assuming column names were originally numbers)\n",
    "df_final[\"timestamp\"] = df_final[\"timestamp\"].astype(int)\n",
    "\n",
    "print(f'{np.where(label)[0]}')\n",
    "px.line(df_final, x='timestamp', y=['X','output'], animation_frame='sensor_id', width=1000, range_y=[-10,35]).show()\n",
    "\n",
    "\n",
    "fig = px.line(y=[label*scores.max()*0.75, scores], width=1000, markers=True)  # Add markers\n",
    "fig.update_traces(line=dict(width=0.5), marker={'size':5})  # Make line thin\n",
    "fig.show()\n",
    "\n",
    "px.line(df_final[df_final.sensor_id.isin(np.where(label)[0])], x='timestamp', y=['X','output'], animation_frame='sensor_id', width=1000, range_y=[-10,35]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = fd.NNGraph(pd.DataFrame(data=dataset['pos'], columns=['easting','northing']), radius=15)\n",
    "utils.plotly_signal(G, X[:,-1].cpu().numpy(), width=500, height=300)\n",
    "utils.plotly_signal(G, label, width=500, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.GCNencoder([15,12,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_classes = [models.GCN2MLP, models.AE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(model, tuple(possible_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygsp.graphs.NNGraph(dataset['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = fd.NNGraph(pd.DataFrame(dataset['pos'], columns=['easting','northing']), radius=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import dense_to_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['edge_weight'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(models, 'AE')([2, 2, 2, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsensors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
