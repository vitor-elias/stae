{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import contextily as cx\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pygsp\n",
    "import warnings, re\n",
    "\n",
    "from shapely.geometry import MultiPoint\n",
    "from sklearn.cluster import KMeans\n",
    "from tsmoothie import LowessSmoother, ExponentialSmoother\n",
    "from pyprojroot import here\n",
    "from scipy.spatial import ConvexHull\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "import source.nn.models as models\n",
    "import source.utils.utils as utils\n",
    "import source.utils.fault_detection as fd\n",
    "\n",
    "from source.utils.utils import roc_params, compute_auc, get_auc, best_mcc, best_f1score, otsuThresholding\n",
    "from source.utils.utils import synthetic_timeseries\n",
    "from source.utils.utils import plotly_signal\n",
    "\n",
    "from importlib import reload\n",
    "models = reload(models)\n",
    "utils = reload(utils)\n",
    "fd = reload(fd)\n",
    "\n",
    "from pyprojroot import here\n",
    "root_dir = str(here())\n",
    "\n",
    "insar_dir = os.path.expanduser('~/data/raw/')\n",
    "data_path = root_dir + '/data/interim/'\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def interpolate_displacement(df):\n",
    "    interpolated_df = df.set_index('timestamp').resample('6D').ffill()\n",
    "    interpolated_df['displacement'] = (\n",
    "                                       df[['timestamp','displacement']].set_index('timestamp')\n",
    "                                                                       .resample('6D')\n",
    "                                                                       .interpolate(method='linear')\n",
    "                                      )\n",
    "    return interpolated_df\n",
    "\n",
    "def smoothing(frac):\n",
    "    def smoothing_(x):\n",
    "        lowess_smoother = LowessSmoother(smooth_fraction=frac, iterations=1) #0.075 \n",
    "        lowess_smoother.smooth(x)\n",
    "        return lowess_smoother.smooth_data[0]\n",
    "    return smoothing_\n",
    "\n",
    "\n",
    "def preprocess(df_orig):\n",
    "    df = df_orig.copy()\n",
    "    df = (df.groupby('pid', as_index=False)\n",
    "                    .apply(interpolate_displacement)\n",
    "                    .reset_index().drop('level_0', axis=1)\n",
    "                    )\n",
    "\n",
    "    df['smoothed'] = df.groupby('pid',as_index=False).displacement.transform(smoothing(50/df.timestamp.nunique()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def resample_df(df):\n",
    "    df_resampled = df.set_index('timestamp').resample('6D').ffill()\n",
    "    return df_resampled\n",
    "\n",
    "def interpolate_signals(df_orig):\n",
    "    df = df_orig.set_index('timestamp').resample('6D').ffill()\n",
    "    df[['displacement','signal']] = ( df_orig[['timestamp','displacement', 'signal']].set_index('timestamp')\n",
    "                                      .resample('6D')\n",
    "                                      .interpolate(method='linear')\n",
    "    )\n",
    "\n",
    "    # Apply smoothing to 'displacement'\n",
    "    smooth_func = smoothing(frac=50 / df.index.nunique())\n",
    "    df['smoothed'] = smooth_func(df['signal'].values)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def evo(x):\n",
    "    return np.abs(x.values[-1] - x.values[0])\n",
    "\n",
    "def grad(x):\n",
    "    return np.max(np.gradient(x.values))\n",
    "\n",
    "\n",
    "# Function to calculate the area of the convex hull\n",
    "def convex_hull_area(df):\n",
    "    points = df[['easting', 'northing']].values\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORMATTING DATA\n",
    "\n",
    "Requires insar datasets to be saved as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSLO\n",
    "df_orig = pd.read_csv(insar_dir + \"2022/L2B_066_0717_IW3_VV.csv\")\n",
    "lat_min, lat_max, lon_min, lon_max = (59.91, 59.925, 10.69, 10.73)\n",
    "\n",
    "# SELECT AND FORMAT DATA\n",
    "df = df_orig.copy()\n",
    "\n",
    "df = df[ (df.longitude>lon_min) & (df.longitude<=lon_max) &\n",
    "            (df.latitude>lat_min) & (df.latitude<=lat_max)  ]\n",
    "\n",
    "# Selection relevant columns\n",
    "date_cols = sorted([col for col in df.columns if \"20\" in col]) #columns named after timestamps\n",
    "keep_cols = date_cols #list with variables to keep from dataframe\n",
    "id_cols = ['pid', 'latitude', 'longitude', 'easting', 'northing', 'mean_velocity']\n",
    "keep_cols.extend(id_cols)\n",
    "df = df[keep_cols]  #replacing old df for memory efficiency\n",
    "\n",
    "# Formatting from wide to tall dataframe\n",
    "# Uses a single column for timestamp and a column for displacement\n",
    "# Number of rows = number of pixels * number of timestamps\n",
    "df = df.melt(id_vars=id_cols, value_vars=date_cols,\n",
    "                var_name='timestamp', value_name='displacement').sort_values('pid')\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "df.sort_values(['pid','timestamp'], inplace=True)\n",
    "\n",
    "df.to_parquet('/home/vitorro/Repositories/stae/data/interim/df_Oslo.parq')\n",
    "print(f'{df.pid.nunique()} pids')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALMO\n",
    "df_orig = pd.read_csv(insar_dir + \"2021/066_0742_iw1_vv.csv\")\n",
    "# lat_min, lat_max, lon_min, lon_max = (55.575, 55.60, 12.9,13.0)\n",
    "# lat_min, lat_max, lon_min, lon_max = (55.55, 55.575, 12.9,13.0)\n",
    "lat_min, lat_max, lon_min, lon_max = (55.55, 55.60, 12.9,13.0)\n",
    "\n",
    "# SELECT AND FORMAT DATA\n",
    "df = df_orig.copy()\n",
    "\n",
    "df = df[ (df.longitude>lon_min) & (df.longitude<=lon_max) &\n",
    "            (df.latitude>lat_min) & (df.latitude<=lat_max)  ]\n",
    "\n",
    "# Selection relevant columns\n",
    "date_cols = sorted([col for col in df.columns if \"20\" in col]) #columns named after timestamps\n",
    "keep_cols = date_cols #list with variables to keep from dataframe\n",
    "id_cols = ['pid', 'latitude', 'longitude', 'easting', 'northing', 'mean_velocity']\n",
    "keep_cols.extend(id_cols)\n",
    "df = df[keep_cols]  #replacing old df for memory efficiency\n",
    "\n",
    "# Formatting from wide to tall dataframe\n",
    "# Uses a single column for timestamp and a column for displacement\n",
    "# Number of rows = number of pixels * number of timestamps\n",
    "df = df.melt(id_vars=id_cols, value_vars=date_cols,\n",
    "                var_name='timestamp', value_name='displacement').sort_values('pid')\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "# OLD DATASET ONLY HAS GOOD DATA AFTER JUNE 2016\n",
    "df = df[df.timestamp>='2016-06-01'].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sort_values(['pid','timestamp'], inplace=True)\n",
    "\n",
    "df.to_parquet('/home/vitorro/Repositories/stae/data/interim/df_Malmo.parq')\n",
    "print(f'{df.pid.nunique()} pids')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECTING CLEAN PARTS OF THE DATA\n",
    "\n",
    "Preprocesses the data to interpolate and denoise  \n",
    "Extracts some simple features  \n",
    "Saves the dataset  \n",
    "Runs app for dataset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = (\n",
    "            df.drop_duplicates('pid')[['pid','latitude','longitude','easting','northing']]\n",
    "            .merge(df.groupby('pid', as_index=False).smoothed.agg([np.ptp, evo, grad]), on='pid', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/vitorro/Repositories/stae/data/interim/df_Oslo_features.parq'\n",
    "df_test.to_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE APP ALLOWS THE SELECTION OF SUBDATASETS BASED ON THE FEATURES\n",
    "data_path = '/home/vitorro/Repositories/stae/data/interim/df_Oslo_features.parq'\n",
    "app_process = subprocess.run(['python', '../source/utils/dash_select.py', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE ANOMALOUS DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geological anomaly (affected area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedata_path = root_dir + \"/data/interim/TGRS_basedata/\"\n",
    "dataset_path = root_dir + \"/data/datasets/Geological_anomaly/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates dataframes with anoamlous data\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed) # 42 for training, 0 for testing\n",
    "\n",
    "onset_options = [1, 2, 3]\n",
    "transient_options = [1, 2, 3]\n",
    "deformation_options = [1, 2]\n",
    "num_samples = 2\n",
    "anomaly_radius = 40\n",
    "mask_size = 250\n",
    "anomaly_level_factor = 0.75\n",
    "\n",
    "selected_area_filenames = []\n",
    "df_orig_names = []\n",
    "for filename in os.listdir(basedata_path):\n",
    "    if 'area' in filename and filename.endswith('.parq'):\n",
    "        selected_area_filenames.append(filename)\n",
    "        df_orig_names.append(filename.split('.')[0][:-6])\n",
    "\n",
    "df_orig_names = list(set(df_orig_names))\n",
    "df_orig_dict = {name:pd.read_parquet(basedata_path + name + '.parq') for name in df_orig_names}\n",
    "\n",
    "metadata = {\n",
    "    'seed': random_seed,\n",
    "    'onset_options': onset_options,\n",
    "    'transient_options': transient_options,\n",
    "    'deformation_options': deformation_options,\n",
    "    'num_samples': num_samples,\n",
    "    'anomaly_radius': anomaly_radius,\n",
    "    'mask_size': mask_size,\n",
    "    'anomaly_level_factor': anomaly_level_factor,\n",
    "    'basedata_path': basedata_path,\n",
    "    'dataset_path': dataset_path,\n",
    "    'selected_area_filenames': selected_area_filenames,\n",
    "    'df_orig_names': df_orig_names,\n",
    "}\n",
    "torch.save(metadata, dataset_path+\"data_metadata.pt\")\n",
    "\n",
    "for selected_area_filename in selected_area_filenames:\n",
    "\n",
    "    area_name = selected_area_filename.split('.')[0]\n",
    "    \n",
    "    df_selected = pd.read_parquet(basedata_path + selected_area_filename)\n",
    "\n",
    "    mask = ( (df_selected.easting < (df_selected.easting.min()+mask_size))\n",
    "              & (df_selected.northing < (df_selected.northing.min()+mask_size))\n",
    "    )\n",
    "    masked_pids = df_selected[mask].pid.unique()\n",
    "  \n",
    "    df_orig_name = area_name[:-6]\n",
    "    df_orig = df_orig_dict[df_orig_name][df_orig_dict[df_orig_name].pid.isin(masked_pids)].copy()\n",
    "\n",
    "    df_area = []\n",
    "    for onset in onset_options:\n",
    "        for transient in transient_options:\n",
    "            for deformation in deformation_options:\n",
    "                for seed in range(num_samples):\n",
    "\n",
    "                    df = df_orig.copy()\n",
    "                    df = df.groupby('pid', as_index=False).apply(resample_df).reset_index().drop('level_0', axis=1)\n",
    "\n",
    "                    node_positions = df.drop_duplicates('pid')[['easting','northing']].values\n",
    "                    anomaly_matrix, label = utils.add_anomaly(node_positions, df.timestamp.nunique(),\n",
    "                                                              anomaly_radius,\n",
    "                                                              onset,\n",
    "                                                              transient,\n",
    "                                                              deformation)\n",
    "                    anomaly_level = df_orig.groupby('pid').displacement.agg(np.ptp).quantile(anomaly_level_factor)\n",
    "                    df['signal'] = df.displacement + anomaly_level*anomaly_matrix.reshape((-1,))\n",
    "                    df['label'] = label.reshape((-1,))\n",
    "\n",
    "                    df_stamps = df[df.timestamp.isin(df_orig.timestamp.unique())]\n",
    "                    df[['displacement','signal','smoothed']] =  (df_stamps.groupby('pid', as_index=False)\n",
    "                                                                 .apply(interpolate_signals, include_groups=True)\n",
    "                                                                 .reset_index(drop=True)\n",
    "                                                                 [['displacement','signal','smoothed']]\n",
    "                    )\n",
    "\n",
    "                    df['original'] = 0\n",
    "                    df.loc[df.timestamp.isin(df_orig.timestamp.unique()), 'original'] = 1\n",
    "\n",
    "\n",
    "                    df['onset'] = onset\n",
    "                    df['transient'] = transient\n",
    "                    df['deformation'] = deformation\n",
    "                    df['seed'] = seed\n",
    "\n",
    "                    df_area.append(df)\n",
    "                    \n",
    "\n",
    "    df_area = pd.concat(df_area)\n",
    "            \n",
    "    filename = f'GA_{area_name[3:]}.parq'\n",
    "    df_area.to_parquet(dataset_path + filename)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats a file with the dataset and metadata ready to be used with pytorch \n",
    "\n",
    "def format_datasets(dataset_path, data='signal', original=1):\n",
    "\n",
    "    onset_options = [1, 2, 3]\n",
    "    transient_options = [1, 2, 3]\n",
    "    deformation_options = [1, 2]\n",
    "    num_samples = 2\n",
    "\n",
    "    metadata = {\n",
    "        'onset_options': onset_options,\n",
    "        'transient_options': transient_options,\n",
    "        'deformation_options': deformation_options,\n",
    "        'num_samples': num_samples,\n",
    "        'dataset_path': dataset_path,\n",
    "    }    \n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith('.parq'):\n",
    "            df = pd.read_parquet(dataset_path+filename)\n",
    "\n",
    "            G = fd.NNGraph(df.drop_duplicates(['pid'])[['easting','northing']], radius=15)\n",
    "            edge_index, edge_weight = dense_to_sparse(torch.tensor(G.W.toarray()).float())\n",
    "            \n",
    "            print('file: ' + filename)\n",
    "            for onset in onset_options:\n",
    "                for transient in transient_options:\n",
    "                    for deformation in deformation_options:\n",
    "                        for seed in range(num_samples):\n",
    "                            df_sample = df[(df.onset==onset) & (df.transient==transient) &\n",
    "                                           (df.deformation==deformation) & (df.seed==seed) & (df.original==original)]\n",
    "                            datasets.append(\n",
    "                                {'metadata':{'filename':filename, 'onset':onset, 'transient':transient,\n",
    "                                             'deformation':deformation, 'seed':seed,\n",
    "                                             'data':data, 'original':original},\n",
    "                                 'data': df_sample[data].values.reshape((df_sample.pid.nunique(),-1)),\n",
    "                                 'label': df_sample.label.values.reshape((df_sample.pid.nunique(),-1)),\n",
    "                                 'pos':df_sample.drop_duplicates(['pid'])[['easting','northing']].values,\n",
    "                                 'G':G,\n",
    "                                 'edge_index':edge_index,\n",
    "                                 'edge_weight':edge_weight,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "    return datasets, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, metadata = format_datasets(dataset_path+'Test/')\n",
    "torch.save(datasets, dataset_path+\"Test/dataset.pt\")\n",
    "torch.save(metadata, dataset_path+\"Test/dataset_metadata.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase anomaly (affected pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedata_path = root_dir + \"/data/interim/TGRS_basedata/\"\n",
    "dataset_path = root_dir + \"/data/datasets/Phase_anomaly/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates dataframes with anoamlous data\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed) # 42 for training, 0 for testing\n",
    "\n",
    "num_samples = 20\n",
    "anomalous_nodes = 5\n",
    "mask_size = 250\n",
    "\n",
    "selected_area_filenames = []\n",
    "df_orig_names = []\n",
    "for filename in os.listdir(basedata_path):\n",
    "    if 'area' in filename and filename.endswith('.parq'):\n",
    "        selected_area_filenames.append(filename)\n",
    "        df_orig_names.append(filename.split('.')[0][:-6])\n",
    "\n",
    "df_orig_names = list(set(df_orig_names))\n",
    "df_orig_dict = {name:pd.read_parquet(basedata_path + name + '.parq') for name in df_orig_names}\n",
    "\n",
    "metadata = {\n",
    "    'num_samples': num_samples,\n",
    "    'anomalous_nodes': anomalous_nodes,\n",
    "    'mask_size': mask_size,\n",
    "    'basedata_path': basedata_path,\n",
    "    'dataset_path': dataset_path,\n",
    "    'selected_area_filenames': selected_area_filenames,\n",
    "    'df_orig_names': df_orig_names,\n",
    "}\n",
    "torch.save(metadata, dataset_path+\"data_metadata.pt\")\n",
    "\n",
    "for selected_area_filename in selected_area_filenames:\n",
    "\n",
    "    area_name = selected_area_filename.split('.')[0]\n",
    "    \n",
    "    df_selected = pd.read_parquet(basedata_path + selected_area_filename)\n",
    "\n",
    "    mask = ( (df_selected.easting < (df_selected.easting.min()+mask_size))\n",
    "              & (df_selected.northing < (df_selected.northing.min()+mask_size))\n",
    "    )\n",
    "    masked_pids = df_selected[mask].pid.unique()\n",
    "  \n",
    "    df_orig_name = area_name[:-6]\n",
    "    df_orig = df_orig_dict[df_orig_name][df_orig_dict[df_orig_name].pid.isin(masked_pids)].copy()\n",
    "\n",
    "    df_area = []\n",
    "\n",
    "    for seed in range(num_samples):\n",
    "\n",
    "        df = df_orig.copy()\n",
    "        df = df.groupby('pid', as_index=False).apply(resample_df).reset_index().drop('level_0', axis=1)\n",
    "\n",
    "        node_positions = df.drop_duplicates('pid')[['easting','northing']].values\n",
    "        anomaly_matrix, label = utils.add_phase_anomaly(node_positions, df.timestamp.nunique(), anomalous_nodes)\n",
    "        df['anomaly'] = anomaly_matrix.reshape((-1,))\n",
    "        df['label'] = label.reshape((-1,))\n",
    "        df['signal'] = df.displacement\n",
    "        df.loc[df['label'] == 1, 'signal'] = df['anomaly']\n",
    "\n",
    "        df_stamps = df[df.timestamp.isin(df_orig.timestamp.unique())]\n",
    "        df[['displacement','signal','smoothed']] =  (df_stamps.groupby('pid', as_index=False)\n",
    "                                                        .apply(interpolate_signals)\n",
    "                                                        .reset_index(drop=True)\n",
    "                                                        [['displacement','signal','smoothed']]\n",
    "        )\n",
    "\n",
    "        df['original'] = 0\n",
    "        df.loc[df.timestamp.isin(df_orig.timestamp.unique()), 'original'] = 1\n",
    "\n",
    "        df['seed'] = seed\n",
    "\n",
    "        df_area.append(df)\n",
    "                    \n",
    "\n",
    "    df_area = pd.concat(df_area)\n",
    "            \n",
    "    filename = f'PA_{area_name[3:]}.parq'\n",
    "    df_area.to_parquet(dataset_path + filename)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_PA(dataset_path, data='signal', original=1):\n",
    "\n",
    "    num_samples =  10\n",
    "\n",
    "    metadata = {\n",
    "        'num_samples': num_samples,\n",
    "        'dataset_path': dataset_path,\n",
    "    }    \n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith('.parq'):\n",
    "            df = pd.read_parquet(dataset_path+filename)\n",
    "\n",
    "            G = fd.NNGraph(df.drop_duplicates(['pid'])[['easting','northing']], radius=15)\n",
    "            edge_index, edge_weight = dense_to_sparse(torch.tensor(G.W.toarray()).float())\n",
    "            \n",
    "            print('file: ' + filename)\n",
    "            for seed in range(num_samples):\n",
    "                df_sample = df[(df.seed==seed) & (df.original==original)]\n",
    "                datasets.append(\n",
    "                    {'metadata':{'filename':filename, 'seed':seed,\n",
    "                                    'data':data, 'original':original},\n",
    "                        'data': df_sample[data].values.reshape((df_sample.pid.nunique(),-1)),\n",
    "                        'label': df_sample.label.values.reshape((df_sample.pid.nunique(),-1)),\n",
    "                        'pos':df_sample.drop_duplicates(['pid'])[['easting','northing']].values,\n",
    "                        'G':G,\n",
    "                        'edge_index':edge_index,\n",
    "                        'edge_weight':edge_weight,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return datasets, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, metadata = get_datasets_PA(dataset_path + 'Training/')\n",
    "torch.save(datasets, dataset_path+\"Training/dataset.pt\")\n",
    "torch.save(metadata, dataset_path+\"Training/dataset_metadata.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both geological and phase anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedata_path = root_dir + \"/data/interim/TGRS_basedata/\"\n",
    "dataset_path = root_dir + \"/data/datasets/EGMS_anomaly/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates dataframes with anoamlous data\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed) # 42 for training, 0 for testing\n",
    "\n",
    "onset_options = [1, 2, 3]\n",
    "transient_options = [1, 2, 3]\n",
    "deformation_options = [1, 2]\n",
    "num_samples = 2\n",
    "anomaly_radius = 40\n",
    "mask_size = 250\n",
    "anomaly_level_factor = 0.75\n",
    "anomalous_nodes = 5\n",
    "\n",
    "selected_area_filenames = []\n",
    "df_orig_names = []\n",
    "for filename in os.listdir(basedata_path):\n",
    "    if 'area' in filename and filename.endswith('.parq'):\n",
    "        selected_area_filenames.append(filename)\n",
    "        df_orig_names.append(filename.split('.')[0][:-6])\n",
    "\n",
    "df_orig_names = list(set(df_orig_names))\n",
    "df_orig_dict = {name:pd.read_parquet(basedata_path + name + '.parq') for name in df_orig_names}\n",
    "\n",
    "metadata = {\n",
    "    'seed': random_seed,\n",
    "    'onset_options': onset_options,\n",
    "    'transient_options': transient_options,\n",
    "    'deformation_options': deformation_options,\n",
    "    'num_samples': num_samples,\n",
    "    'anomaly_radius': anomaly_radius,\n",
    "    'mask_size': mask_size,\n",
    "    'anomaly_level_factor': anomaly_level_factor,\n",
    "    'anomalous_nodes': anomalous_nodes,\n",
    "    'basedata_path': basedata_path,\n",
    "    'dataset_path': dataset_path,\n",
    "    'selected_area_filenames': selected_area_filenames,\n",
    "    'df_orig_names': df_orig_names,\n",
    "}\n",
    "torch.save(metadata, dataset_path+\"data_metadata.pt\")\n",
    "\n",
    "for selected_area_filename in selected_area_filenames:\n",
    "\n",
    "    area_name = selected_area_filename.split('.')[0]\n",
    "    \n",
    "    df_selected = pd.read_parquet(basedata_path + selected_area_filename)\n",
    "\n",
    "    mask = ( (df_selected.easting < (df_selected.easting.min()+mask_size))\n",
    "              & (df_selected.northing < (df_selected.northing.min()+mask_size))\n",
    "    )\n",
    "    masked_pids = df_selected[mask].pid.unique()\n",
    "  \n",
    "    df_orig_name = area_name[:-6]\n",
    "    df_orig = df_orig_dict[df_orig_name][df_orig_dict[df_orig_name].pid.isin(masked_pids)].copy()\n",
    "\n",
    "    df_area = []\n",
    "    for onset in onset_options:\n",
    "        for transient in transient_options:\n",
    "            for deformation in deformation_options:\n",
    "                for seed in range(num_samples):\n",
    "\n",
    "                    df = df_orig.copy()\n",
    "                    df = df.groupby('pid', as_index=False).apply(resample_df).reset_index().drop('level_0', axis=1)\n",
    "\n",
    "                    node_positions = df.drop_duplicates('pid')[['easting','northing']].values\n",
    "                    anomaly_matrix, label = utils.add_anomaly(node_positions, df.timestamp.nunique(),\n",
    "                                                              anomaly_radius,\n",
    "                                                              onset,\n",
    "                                                              transient,\n",
    "                                                              deformation)\n",
    "                    anomaly_level = df_orig.groupby('pid').displacement.agg(np.ptp).quantile(anomaly_level_factor)\n",
    "                    df['signal'] = df.displacement + anomaly_level*anomaly_matrix.reshape((-1,))\n",
    "                    df['label'] = label.reshape((-1,))\n",
    "\n",
    "\n",
    "                    mask = np.where(label.any(axis=1))[0]\n",
    "                    ####\n",
    "                    anomaly_matrix, label = utils.add_phase_anomaly_masked(node_positions,\n",
    "                                                                           df.timestamp.nunique(),\n",
    "                                                                           anomalous_nodes,\n",
    "                                                                           mask)\n",
    "                    df['anomaly'] = anomaly_matrix.reshape((-1,))\n",
    "                    df['phase_label'] = label.reshape((-1,))\n",
    "                    df.loc[df['phase_label'] == 1, 'signal'] = df['anomaly']\n",
    "                    df.loc[df['phase_label'] == 1, 'label'] = 2\n",
    "                    ####\n",
    "\n",
    "                    df_stamps = df[df.timestamp.isin(df_orig.timestamp.unique())]\n",
    "                    df[['displacement','signal','smoothed']] =  (df_stamps.groupby('pid', as_index=False)\n",
    "                                                                 .apply(interpolate_signals, include_groups=True)\n",
    "                                                                 .reset_index(drop=True)\n",
    "                                                                 [['displacement','signal','smoothed']]\n",
    "                    )\n",
    "\n",
    "                    df['original'] = 0\n",
    "                    df.loc[df.timestamp.isin(df_orig.timestamp.unique()), 'original'] = 1\n",
    "\n",
    "\n",
    "                    df['onset'] = onset\n",
    "                    df['transient'] = transient\n",
    "                    df['deformation'] = deformation\n",
    "                    df['seed'] = seed\n",
    "\n",
    "                    df_area.append(df)\n",
    "                    \n",
    "\n",
    "    df_area = pd.concat(df_area)\n",
    "            \n",
    "    filename = f'EA_{area_name[3:]}.parq'\n",
    "    df_area.to_parquet(dataset_path + filename)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, metadata = format_datasets(dataset_path+'Test/')\n",
    "torch.save(datasets, dataset_path+\"Test/dataset.pt\")\n",
    "torch.save(metadata, dataset_path+\"Test/dataset_metadata.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_area.drop_duplicates(['pid','timestamp']).query('label==2'), x='timestamp', y='signal', animation_frame='pid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_area.drop_duplicates('pid'), x='easting', y='northing', color='label', hover_data=['pid'],\n",
    "           color_continuous_scale=px.colors.sequential.Plasma, title='EGMS Anomaly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(dataset_path + 'Oslo/training/ds_df_Oslo_area1.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_params = {'edge_color':'darkgray', 'edge_width':1,'vertex_color':'black', 'vertex_size':25}\n",
    "G = fd.NNGraph(df.drop_duplicates('pid')[['easting','northing']], radius=15, plotting_params=plotting_params)\n",
    "\n",
    "# can you zoom in in the plot?\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# G.plot(plot_name='', ax=ax)\n",
    "G.plot_signal(df.drop_duplicates('pid').displacement.values, ax=ax, colorbar=True, plot_name='', show_edges=True)\n",
    "#remove box from ax\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False) \n",
    "#remove ticks from ax\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "#remove labels from ax\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "# #change the colorbar map\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label('Displacement (mm)', rotation=90, labelpad=10, fontsize=14)\n",
    "ax.collections[0].set_cmap('coolwarm')\n",
    "\n",
    "# #remove box around colorbar\n",
    "cbar.outline.set_visible(False)\n",
    "\n",
    "\n",
    "#change colorbar tick font size\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "# Add contextily basemap\n",
    "cx.add_basemap(plt.gca(), crs='EPSG:3035', source=cx.providers.CartoDB.Positron)\n",
    "# cx.add_basemap(ax, crs='epsg:25833', source=cx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# plt.savefig('/home/vitorro/Repositories/stae/outputs/figs/EGMS_graph_example_plt.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df[df.pid==df.pid.unique()[0]].query('original==1'), x='timestamp', y='displacement', width=800,\n",
    "                 labels={'displacement':'Displacement (mm)', 'timestamp':'Date'})\n",
    "\n",
    "# Make the background transparent\n",
    "# fig.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',)\n",
    "fig.update_xaxes(tickformat='%Y-Q%q', dtick='M3', title_font={'size': 16}, tickfont={'size': 14}) \n",
    "fig.update_yaxes(title_font={'size': 18}, tickfont={'size': 14})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.query('onset==1 and transient==1 and deformation==1 and seed==0')\n",
    "X = df_sample.signal.values.reshape((df_sample.pid.nunique(),-1))\n",
    "px.line(X[3,:],width=1000).show()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "dataset_name = 'EGMS_anomaly/'\n",
    "datasets = torch.load(dataset_path + f'{dataset_name}/Test/dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = datasets[0]['label'].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=datasets[0]['pos'][:,0], y=datasets[0]['pos'][:,1], color=label, color_continuous_scale=px.colors.sequential.Plasma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[20]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "dataset_name = 'EGMS_anomaly/'\n",
    "datasets = torch.load(dataset_path + f'{dataset_name}/Test/dataset.pt')\n",
    "dataset = datasets[20]\n",
    "\n",
    "label = dataset['label'].max(axis=1)\n",
    "\n",
    "# Create figure and plot\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Create custom colormap for 3 categories\n",
    "colors = [(0.5, 0.5, 0.8), (0.05, 0.25, 0.25), (0.55, 0.55, 0.05)]  # blue, orange, green\n",
    "categories = ['Normal', 'Geological', 'Phase']\n",
    "scatter_plots = []\n",
    "\n",
    "# Plot each category separately to create legend entries\n",
    "for i, cat in enumerate(np.unique(label)):\n",
    "    mask = label == cat\n",
    "    scatter = ax.scatter(dataset['pos'][mask,0], dataset['pos'][mask,1], \n",
    "                        c=[colors[int(cat)]], label=categories[int(cat)], \n",
    "                        s=25, alpha=0.7)\n",
    "\n",
    "# Remove box and ticks\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title='Label', frameon=True, loc='lower left')\n",
    "\n",
    "# Add contextily basemap\n",
    "cx.add_basemap(plt.gca(), crs='EPSG:3035', source=cx.providers.CartoDB.Positron)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5,5), tight_layout=True)\n",
    "\n",
    "# Get indices where label is 1 and 2\n",
    "idx1 = np.where(label == 1)[0] \n",
    "idx2 = np.where(label == 2)[0]\n",
    "\n",
    "if len(idx1) > 0:\n",
    "    X1 = dataset['data'][idx1[0],:]\n",
    "    ax1.plot(X1, label='Geological anomaly')\n",
    "    ax1.legend(frameon=False, fontsize=12, loc='lower right')\n",
    "    ax1.set_xticks([])\n",
    "    # ax1.grid(True)\n",
    "\n",
    "if len(idx2) > 0:    \n",
    "    X2 = dataset['data'][idx2[0],:]\n",
    "    ax2.plot(X2, label='Phase anomaly')\n",
    "    ax2.legend(frameon=False, fontsize=12)\n",
    "    # ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "dataset_name = 'EGMS_anomaly/'\n",
    "datasets = torch.load(dataset_path + f'{dataset_name}/Test/dataset.pt')\n",
    "dataset = datasets[20]\n",
    "\n",
    "label = dataset['label'].max(axis=1)\n",
    "\n",
    "# Create figure with 1x3 subplot layout \n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Scatter plot\n",
    "colors = [(0.5, 0.5, 0.8), (0.05, 0.25, 0.25), (0.55, 0.55, 0.05)]\n",
    "categories = ['Normal', 'Geological', 'Phase']\n",
    "\n",
    "for i, cat in enumerate(np.unique(label)):\n",
    "    mask = label == cat\n",
    "    ax1.scatter(dataset['pos'][mask,0], dataset['pos'][mask,1], \n",
    "                c=[colors[int(cat)]], label=categories[int(cat)], \n",
    "                s=25, alpha=0.7)\n",
    "\n",
    "# Clean up scatter plot\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax1.tick_params(axis='both', which='both', length=0)\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_xticklabels([])\n",
    "ax1.legend(title='', frameon=True, loc='upper right', fontsize=12)\n",
    "\n",
    "# Add basemap\n",
    "cx.add_basemap(ax1, crs='EPSG:3035', source=cx.providers.CartoDB.Positron)\n",
    "\n",
    "# Time series plots\n",
    "idx1 = np.where(label == 1)[0] \n",
    "idx2 = np.where(label == 2)[0]\n",
    "\n",
    "\n",
    "X1 = dataset['data'][idx1[0],:]\n",
    "ax2.plot(X1, color=colors[1])\n",
    "ax2.set_title('Geological anomaly', fontsize=12, loc='left')\n",
    "ax2.set_xticks([])\n",
    "\n",
    "X2 = dataset['data'][idx2[0],:]\n",
    "ax3.plot(X2, color=colors[2])\n",
    "ax3.set_title('Phase anomaly', fontsize=14, loc='left')\n",
    "ax3.tick_params(axis='x', direction='in')\n",
    "ax3.set_xticks([])\n",
    "ax3.set_xlabel('Time', fontsize=14)\n",
    "\n",
    "# Configure time series axes\n",
    "for ax in [ax2, ax3]:\n",
    "    # Hide default y-axis ticks\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "ax2.set_xlim([0,325])\n",
    "ax2.annotate('0', xy=(325, 0), xycoords='data', ha='right', fontsize=12)\n",
    "ax2.annotate('15', xy=(325, 15), xycoords='data', ha='right', fontsize=12)\n",
    "ax2.annotate('30', xy=(325, 30), xycoords='data', ha='right', fontsize=12)\n",
    "\n",
    "ax3.set_xlim([0,325])\n",
    "ax3.annotate('0', xy=(325, -5), xycoords='data', ha='right', fontsize=12)\n",
    "ax3.annotate('-40', xy=(325, -40), xycoords='data', ha='right', fontsize=12)\n",
    "ax3.annotate('-80', xy=(325, -80), xycoords='data', ha='right', fontsize=12)\n",
    "\n",
    "fig.text(0.99, 0.5, 'Displacement (mm)', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "plt.tight_layout(pad=1.0)\n",
    "\n",
    "plt.savefig('/home/vitorro/Repositories/stae/outputs/figs/EGMS_anomaly_example_plt.png', dpi=300,bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Oslo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_Oslo = pd.read_parquet('/home/vitorro/Repositories/stae/data/interim/df_Oslo.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Oslo[['easting','northing']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_Oslo.drop_duplicates('pid')[['easting','northing']].values[:,0] - df_Oslo.drop_duplicates('pid')[['easting','northing']].values[:,0].min(),\n",
    "            df_Oslo.drop_duplicates('pid')[['easting','northing']].values[:,1] - df_Oslo.drop_duplicates('pid')[['easting','northing']].values[:,1].min(),\n",
    "            s=1, alpha=0.5, c='black')\n",
    "cx.add_basemap(plt.gca(), crs='EPSG:3035', source=cx.providers.CartoDB.Positron)\n",
    "plt.title('Oslo area')\n",
    "plt.xlabel('Easting (m)')\n",
    "plt.ylabel('Northing (m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "for east in range(int(df_Oslo.easting.min()), int(df_Oslo.easting.max()), 200):\n",
    "    for north in range(int(df_Oslo.northing.min()), int(df_Oslo.northing.max()), 200):\n",
    "        mask_size = 250\n",
    "        mask = ( (df_Oslo.easting>east) & (df_Oslo.easting < (east+mask_size)) &\n",
    "                 (df_Oslo.northing>north) & (df_Oslo.northing < (north+mask_size))\n",
    "        )\n",
    "        masked_pids = df_Oslo[mask].pid.unique()\n",
    "\n",
    "        # Skip if number of pids is greater than 50\n",
    "        if len(masked_pids) < 50:\n",
    "            continue\n",
    "\n",
    "        df_mask = df_Oslo[df_Oslo.pid.isin(masked_pids)].copy()\n",
    "\n",
    "        while df_mask.drop_duplicates('pid')[['easting','northing']].duplicated(['easting','northing']).values.any():\n",
    "            fix_pid = df_mask.drop_duplicates('pid')[df_mask.drop_duplicates('pid')[['easting','northing']].duplicated(['easting','northing'], keep='first').values].pid.unique()\n",
    "            for pid in fix_pid:\n",
    "                df_mask.loc[df_mask.pid==pid, 'easting'] += np.random.uniform(-0.5, 0.5)\n",
    "\n",
    "        G = fd.NNGraph(df_mask.drop_duplicates('pid')[['easting','northing']], radius=15)\n",
    "        edge_index, edge_weight = dense_to_sparse(torch.tensor(G.W.toarray()).float())\n",
    "\n",
    "        datasets.append(\n",
    "            {'metadata':{'easting':east, 'northing':north, 'mask_size':mask_size},\n",
    "             'data': df_mask.displacement.values.reshape((df_mask.pid.nunique(),-1)),\n",
    "             'pos':df_mask.drop_duplicates(['pid'])[['easting','northing']].values,\n",
    "             'coords':df_mask.drop_duplicates(['pid'])[['latitude','longitude']].values,\n",
    "             'pid':df_mask.drop_duplicates(['pid']).pid.values,\n",
    "             'G':G,\n",
    "             'edge_index':edge_index,\n",
    "             'edge_weight':edge_weight,\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(datasets, '/home/vitorro/Repositories/stae/data/datasets/Real_data/dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d['G'].N for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask[['easting','northing']].duplicated(['easting','northing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.diag(G.W.toarray())>0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsensors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
