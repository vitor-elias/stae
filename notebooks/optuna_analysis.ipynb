{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import contextily as cx\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import torch\n",
    "import joblib\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "from IPython.display import display_html\n",
    "from shapely.geometry import MultiPoint\n",
    "from sklearn.cluster import KMeans\n",
    "from tsmoothie import LowessSmoother, ExponentialSmoother\n",
    "from pyprojroot import here\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import source.nn.models as models\n",
    "import source.utils.utils as utils\n",
    "import source.utils.fault_detection as fd\n",
    "\n",
    "from source.utils.utils import roc_params, compute_auc, get_auc, best_mcc, best_f1score, otsuThresholding\n",
    "from source.utils.utils import synthetic_timeseries\n",
    "from source.utils.utils import plotly_signal\n",
    "\n",
    "from importlib import reload\n",
    "models = reload(models)\n",
    "utils = reload(utils)\n",
    "fd = reload(fd)\n",
    "\n",
    "from pyprojroot import here\n",
    "root_dir = str(here())\n",
    "\n",
    "insar_dir = os.path.expanduser('~/data/raw/')\n",
    "data_path = root_dir + '/data/interim/'\n",
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "graph_models = tuple([models.GCN2MLP, models.GConv2MLP, models.GCNAE, models.GConvAE, models.GUNet])\n",
    "model_names = ['AE', 'GCN2MLP', 'GCNAE', 'GConv2MLP', 'GConvAE', 'GUNet', 'RAE_GRU', 'RAE_LSTM']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_name(model_name, n_nodes, input_dim, params, device='cuda'):\n",
    "    # Determine the model class and parameters\n",
    "    if 'RAE' in model_name:\n",
    "        rnn_type = 'GRU' if 'GRU' in model_name else 'LSTM'\n",
    "        model_params = {\n",
    "            'n_features': n_nodes,\n",
    "            'latent_dim': params['latent_dim'],\n",
    "            'rnn_type': rnn_type,\n",
    "            'rnn_act': 'relu',\n",
    "            'device': device\n",
    "        }\n",
    "        model_class = getattr(models, 'RAE')\n",
    "    elif 'GUNet' in model_name:\n",
    "        model_params = {\n",
    "            'in_channels': input_dim,\n",
    "            'out_channels': input_dim,\n",
    "            'hidden_channels': params['hidden_channels'],\n",
    "            'depth': params['depth'],\n",
    "            'pool_ratios': params['pool_ratios']\n",
    "        }\n",
    "        model_class = getattr(models, 'GUNet')\n",
    "    else:  # Covers AE, GCN, and GConv models\n",
    "        layer_dims = [input_dim]\n",
    "        current_dim = 2 * input_dim\n",
    "        n_layers = params['n_layers']\n",
    "        for i in range(n_layers):\n",
    "            next_dim = params[f'layer_dim_{i}']\n",
    "            layer_dims.append(int(next_dim))\n",
    "            current_dim = next_dim\n",
    "        model_params = {'layer_dims': layer_dims}\n",
    "        model_class = getattr(models, model_name)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = model_class(**model_params).to(device)    \n",
    "\n",
    "    return model\n",
    "\n",
    "def compute_num_parameters(study_file, n_nodes, input_dim, device='cpu'):\n",
    "    # Load the study\n",
    "    study = joblib.load(study_file)\n",
    "    model_name = os.path.basename(study_file)[3:-4]\n",
    "    \n",
    "    results = []\n",
    "    for trial in study.trials:\n",
    "        params = trial.params\n",
    "\n",
    "        model = get_model_from_name(model_name, n_nodes, input_dim, params, device=device)\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        results.append({**trial.params, 'num_parameters': num_params})\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = study.trials_dataframe()\n",
    "    df['num_parameters'] = [r['num_parameters'] for r in results]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Remove default values when extracting parameters from the dictionary\n",
    "def get_model_with_fewest_params(file, n_nodes, input_dim, top_factor=0.995, device='cuda'):\n",
    "    # Compute the study DataFrame with number of parameters\n",
    "    df_study = compute_num_parameters(file, n_nodes, input_dim, device=device)\n",
    "    \n",
    "    # Filter for trials within the top 0.5% of value_auc\n",
    "    top_trials = df_study.query('value_auc > @top_factor * value_auc.max()')\n",
    "    \n",
    "    # Find the trial with the fewest number of parameters\n",
    "    best_trial = top_trials.loc[top_trials['num_parameters'].idxmin()]\n",
    "    \n",
    "    # Extract the trial parameters\n",
    "    trial_params = best_trial.filter(like='params_').to_dict()\n",
    "    trial_params = {key.replace('params_', ''): value for key, value in trial_params.items()}\n",
    "   \n",
    "    model_name = os.path.basename(file)[3:-4]    \n",
    "    model = get_model_from_name(model_name, n_nodes, input_dim, trial_params, device=device)\n",
    "\n",
    "    # Return the model, number of parameters, and AUC value\n",
    "    num_parameters = best_trial['num_parameters']\n",
    "\n",
    "    model_info = {\n",
    "            'model_example': model,\n",
    "            'trial_params': trial_params,\n",
    "            'num_parameters': num_parameters,\n",
    "            # 'auc': auc_value,\n",
    "            # 'auc_list': best_trial['user_attrs_auc_dataset'],\n",
    "        }\n",
    "    return model_name, model_info\n",
    "\n",
    "\n",
    "def train_model(model, X, lr, n_epochs, edge_index=None, edge_weight=None, batch_size=None):\n",
    "    \n",
    "    rng_seed = 0\n",
    "    torch.manual_seed(rng_seed)\n",
    "    torch.cuda.manual_seed(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    model.reset_parameters()\n",
    "\n",
    "    if isinstance(model, models.RAE):\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            output = model(X.T.unsqueeze(0)).squeeze(0).T\n",
    "            loss = criterion(output, X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(model, graph_models):\n",
    "            output = model(X, edge_index=edge_index, edge_weight=edge_weight)\n",
    "        else:\n",
    "            output = model(X)\n",
    "        loss = criterion(output, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return output\n",
    "\n",
    "def pixel_mse(output,X):\n",
    "    point_mse = torch.nn.MSELoss(reduction='none')\n",
    "    return torch.mean(point_mse(output,X),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = root_dir + \"/data/datasets/\"\n",
    "\n",
    "for dataset_name in ['Geological_anomaly', 'EGMS_anomaly']:\n",
    "    print(f\"\\nProcessing dataset {dataset_name}\")\n",
    "\n",
    "    datafile = f'{dataset_name}/Training/dataset.pt'\n",
    "    datasets = torch.load(dataset_path + datafile)\n",
    "\n",
    "    optuna_files = glob.glob(root_dir + f'/outputs/HP_training/{dataset_name}/*.pkl')\n",
    "\n",
    "    n_nodes = int(np.mean([dataset['data'].shape[0] for dataset in datasets]))\n",
    "    input_dim = datasets[0]['data'].shape[1]\n",
    "\n",
    "    print('Creating best model dictionary')\n",
    "    model_dict = {}\n",
    "    for file in optuna_files:\n",
    "        model_name, model_info = get_model_with_fewest_params(file, n_nodes, input_dim, device=device)\n",
    "        model_dict[model_name] = model_info\n",
    "\n",
    "  \n",
    "    # Save the updated model_dict for the current datafile\n",
    "    output_path = os.path.join(root_dir, 'outputs/Optuna_analysis', f'model_dict_{dataset_name}.pkl')\n",
    "    torch.save(model_dict, output_path)\n",
    "    print(f\"\\nSaved model_dict for {datafile} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN Epoch_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Geological_anomaly'\n",
    "epoch_data = torch.load(root_dir + f'/outputs/Optuna_analysis/Epochs_{dataset_name}.pkl')\n",
    "# For each model, generate the mean AUC evolution and mean Loss evolution across the 25 seeds and 108 datasets\n",
    "\n",
    "for model_name in epoch_data.keys():\n",
    "\n",
    "    auc_evolution = []\n",
    "    loss_evolution = []\n",
    "    # Iterate through each dataset\n",
    "    for dataset_idx in range(len(epoch_data[model_name]['auc_evolution'])):\n",
    "        # Iterate through each seed\n",
    "        for seed in range(len(epoch_data[model_name]['auc_evolution'][dataset_idx])):\n",
    "\n",
    "            auc_evolution.append(epoch_data[model_name]['auc_evolution'][dataset_idx][seed])\n",
    "            loss_evolution.append(epoch_data[model_name]['loss_evolution'][dataset_idx][seed])\n",
    "\n",
    "\n",
    "    # Store the mean AUC and Loss values in the epoch_data dictionary\n",
    "    epoch_data[model_name]['mean_auc'] = np.mean(auc_evolution, axis=0)\n",
    "    epoch_data[model_name]['mean_loss'] = np.median(loss_evolution, axis=0)\n",
    "\n",
    "epoch_data_geological = epoch_data\n",
    "\n",
    "\n",
    "dataset_name = 'EGMS_anomaly'\n",
    "epoch_data = torch.load(root_dir + f'/outputs/Optuna_analysis/Epochs_{dataset_name}.pkl')\n",
    "# For each model, generate the mean AUC evolution and mean Loss evolution across the 25 seeds and 108 datasets\n",
    "\n",
    "for model_name in epoch_data.keys():\n",
    "\n",
    "    auc_evolution = []\n",
    "    loss_evolution = []\n",
    "    # Iterate through each dataset\n",
    "    for dataset_idx in range(len(epoch_data[model_name]['auc_evolution'])):\n",
    "        # Iterate through each seed\n",
    "        for seed in range(len(epoch_data[model_name]['auc_evolution'][dataset_idx])):\n",
    "\n",
    "            auc_evolution.append(epoch_data[model_name]['auc_evolution'][dataset_idx][seed])\n",
    "            loss_evolution.append(epoch_data[model_name]['loss_evolution'][dataset_idx][seed])\n",
    "\n",
    "\n",
    "    # Store the mean AUC and Loss values in the epoch_data dictionary\n",
    "    epoch_data[model_name]['mean_auc'] = np.mean(auc_evolution, axis=0)\n",
    "    epoch_data[model_name]['mean_loss'] = np.median(loss_evolution, axis=0)\n",
    "\n",
    "epoch_data_egms = epoch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_data = epoch_data_geological\n",
    "n_cols = 4\n",
    "n_rows = 2\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5))  # Create subplots\n",
    "\n",
    "# Plot for 'mean_loss'\n",
    "variable = 'mean_loss'\n",
    "(xmin, xmax) = (0, 150)  # X-axis limits\n",
    "\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    median_loss = epoch_data[model_name][variable]\n",
    "    axes_flat[idx].plot(median_loss, linewidth=2)\n",
    "\n",
    "    if model_name in ['AE']:\n",
    "        (ymin, ymax) = (10, 100)  # Y-axis limits\n",
    "    else:\n",
    "        (ymin, ymax) = (10, 100)  # Y-axis limits\n",
    "    \n",
    "    # Only show x labels and ticks for bottom row\n",
    "    if idx < len(model_names) - n_cols:  # If not in bottom row\n",
    "        axes_flat[idx].set_xticklabels([])\n",
    "        axes_flat[idx].set_xlabel('')\n",
    "    else:\n",
    "        axes_flat[idx].set_xlabel(\"Epoch\", fontsize=12)\n",
    "        \n",
    "    # Only show y labels and ticks for leftmost column\n",
    "    if idx % n_cols == 0: \n",
    "        axes_flat[idx].set_ylabel(\"Loss\", fontsize=12)\n",
    "        \n",
    "    axes_flat[idx].set_xlim(xmin, xmax)\n",
    "    axes_flat[idx].set_ylim(ymin, ymax)\n",
    "    axes_flat[idx].grid(True, linestyle='--', alpha=0.25)\n",
    "    axes_flat[idx].set_title(f\"{model_name.replace('_','')}\", fontsize=14)\n",
    "    axes_flat[idx].tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root_dir, \"outputs/figs/AUC_evo_Geological.pdf\"), format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_data = epoch_data_egms\n",
    "\n",
    "n_cols = 4\n",
    "n_rows = 2\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5))  # Create subplots\n",
    "\n",
    "# Plot for 'mean_loss'\n",
    "variable = 'mean_loss'\n",
    "(xmin, xmax) = (0, 150)  # X-axis limits\n",
    "\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    median_loss = epoch_data[model_name][variable]\n",
    "    axes_flat[idx].plot(median_loss, linewidth=2)\n",
    "\n",
    "    if model_name in ['AE']:\n",
    "        (ymin, ymax) = (10, 80)  # Y-axis limits\n",
    "    else:\n",
    "        (ymin, ymax) = (70, 90)  # Y-axis limits\n",
    "    \n",
    "    # Only show x labels and ticks for bottom row\n",
    "    if idx < len(model_names) - n_cols:  # If not in bottom row\n",
    "        axes_flat[idx].set_xticklabels([])\n",
    "        axes_flat[idx].set_xlabel('')\n",
    "    else:\n",
    "        axes_flat[idx].set_xlabel(\"Epoch\", fontsize=12)\n",
    "        \n",
    "    # Only show y labels and ticks for leftmost column\n",
    "    if idx % n_cols == 0: \n",
    "        axes_flat[idx].set_ylabel(\"Loss\", fontsize=12)\n",
    "    \n",
    "    axes_flat[idx].set_xlim(xmin, xmax)\n",
    "    axes_flat[idx].set_ylim(ymin, ymax)\n",
    "    axes_flat[idx].grid(True, linestyle='--', alpha=0.25)\n",
    "    axes_flat[idx].set_title(f\"{model_name.replace('_','')}\", fontsize=14)\n",
    "    axes_flat[idx].tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root_dir, \"outputs/figs/AUC_evo_EGMS.pdf\"), format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))  # Create a figure with 1 subplot\n",
    "\n",
    "# Define markers for each line\n",
    "markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p']\n",
    "\n",
    "# Plot for 'mean_loss'\n",
    "variable = 'mean_loss'\n",
    "(xmin, xmax) = (0, 150)  # X-axis limits\n",
    "(ymin, ymax) = (20, 50)  # Y-axis limits\n",
    "x_pos = 60  # Position at the end of the curve\n",
    "\n",
    "y_positions = [epoch_data[model_name][variable][x_pos] for model_name in epoch_data.keys()]\n",
    "ranking = 2 * np.argsort(np.argsort(y_positions)[::-1])\n",
    "x_offset = 20\n",
    "offsets = ranking - np.mean(ranking)\n",
    "\n",
    "for it, (model_name, marker) in enumerate(zip(epoch_data.keys(), markers)):\n",
    "    median_loss = epoch_data[model_name][variable]\n",
    "    line, = ax.plot(median_loss, label=model_name, linewidth=2, marker=marker, markevery=5, markersize=4)\n",
    "    color = line.get_color()  # Get the color of the current line\n",
    "\n",
    "    offset = offsets[it]\n",
    "    y_pos = median_loss[x_pos]\n",
    "\n",
    "    if (y_pos - offset) < ymin:\n",
    "        offset = y_pos - ymin \n",
    "    if offset > y_pos:\n",
    "        offset = 0\n",
    "    if (y_pos - offset) > ymax:\n",
    "        offset = y_pos - ymax + (ymax - ymin) / 25\n",
    "\n",
    "    ax.annotate(model_name.replace(\"_\", \"\"), \n",
    "                xy=(x_pos, y_pos), \n",
    "                xytext=(x_pos + x_offset, y_pos - offset),\n",
    "                arrowprops=dict(arrowstyle='->', color=color),\n",
    "                fontsize=14,\n",
    "                color=color)\n",
    "\n",
    "ax.set_xlabel(\"Epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"Loss\", fontsize=16)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.grid(True, linestyle='--', alpha=0.25)\n",
    "ax.set_title(\"Median Loss Evolution\", fontsize=16)\n",
    "ax.tick_params(axis='both', labelsize=14)  # Change tick label size\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # Create a figure with 1 row and 2 columns\n",
    "\n",
    "# Plot for 'mean_loss'\n",
    "variable = 'mean_loss'\n",
    "(xmin, xmax) = (0, 150)  # X-axis limits\n",
    "(ymin, ymax) = (70, 100)  # Y-axis limits\n",
    "x_pos = 40  # Position at the end of the curve\n",
    "\n",
    "y_positions = [epoch_data[model_name][variable][x_pos] for model_name in epoch_data.keys()]\n",
    "ranking = 1 * np.argsort(np.argsort(y_positions)[::-1])\n",
    "x_offset = 10\n",
    "offsets = ranking - np.mean(ranking)\n",
    "\n",
    "it = 0\n",
    "for model_name in epoch_data.keys():\n",
    "    median_loss = epoch_data[model_name][variable]\n",
    "    axes[0].plot(median_loss, label=model_name, linewidth=2)\n",
    "\n",
    "    offset = offsets[it]\n",
    "    it += 1\n",
    "    y_pos = median_loss[x_pos]\n",
    "\n",
    "    if (y_pos - offset) < ymin:\n",
    "        offset = 0\n",
    "    if offset > y_pos:\n",
    "        offset = 0\n",
    "\n",
    "    axes[0].annotate(model_name.replace(\"_\", \"\"), xy=(x_pos, y_pos), xytext=(x_pos + x_offset, y_pos - offset),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='-'),\n",
    "                     fontsize=12)\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=16)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "axes[0].set_xlim(xmin, xmax)\n",
    "axes[0].set_ylim(ymin, ymax)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.25)\n",
    "axes[0].set_title(\"Median Loss Evolution\", fontsize=16)\n",
    "axes[0].tick_params(axis='both', labelsize=14)  # Change tick label size\n",
    "\n",
    "# Plot for 'mean_auc'\n",
    "variable = 'mean_auc'\n",
    "(xmin, xmax) = (0, 25)  # X-axis limits\n",
    "(ymin, ymax) = (0.7, 1.0)  # Y-axis limits\n",
    "x_pos = 1  # Position at the end of the curve\n",
    "\n",
    "y_positions = [epoch_data[model_name][variable][x_pos] for model_name in epoch_data.keys()]\n",
    "ranking = 0.01 * np.argsort(np.argsort(y_positions)[::-1])\n",
    "x_offset = 4\n",
    "offsets = ranking - np.mean(ranking)\n",
    "\n",
    "it = 0\n",
    "for model_name in epoch_data.keys():\n",
    "    mean_auc = epoch_data[model_name][variable]\n",
    "    axes[1].plot(mean_auc, label=model_name, linewidth=2)\n",
    "\n",
    "    offset = offsets[it]\n",
    "    it += 1\n",
    "    y_pos = mean_auc[x_pos]\n",
    "\n",
    "    if (y_pos - offset) < ymin:\n",
    "        offset = 0\n",
    "\n",
    "    axes[1].annotate(model_name.replace(\"_\", \"\"), xy=(x_pos, y_pos), xytext=(x_pos + x_offset, y_pos - offset),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='-'),\n",
    "                     fontsize=12)\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=16)\n",
    "axes[1].set_ylabel(\"AUC\", fontsize=16)\n",
    "axes[1].set_xlim(xmin, xmax)\n",
    "axes[1].set_ylim(ymin, ymax)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.25)\n",
    "axes[1].set_title(\"Mean AUC Evolution\", fontsize=16)\n",
    "axes[1].set_xticks([0, 5, 10, 15])\n",
    "axes[1].tick_params(axis='both', labelsize=14)  # Change tick label size\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(root_dir, \"outputs/figs/training_Geological.pdf\"), format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Geological_anomaly'\n",
    "datasets = torch.load(dataset_path + f'{dataset_name}/Training/dataset.pt')\n",
    "model_dict = torch.load(root_dir + f'/outputs/Optuna_analysis/model_dict_{dataset_name}.pkl')\n",
    "\n",
    "# Create a list to store all the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name in model_dict.keys():\n",
    "    # Get auc and loss evolution for this model\n",
    "    auc_evolutions = model_dict[model_name]['auc_evolution']\n",
    "    loss_evolutions = model_dict[model_name]['loss_evolution']\n",
    "    \n",
    "    # For each dataset\n",
    "    for dataset_idx in range(len(auc_evolutions)):\n",
    "        # Get the evolution sequences\n",
    "        auc_seq = auc_evolutions[dataset_idx]\n",
    "        loss_seq = loss_evolutions[dataset_idx]\n",
    "        \n",
    "        # For each epoch\n",
    "        for epoch in range(len(auc_seq)):\n",
    "            data.append({\n",
    "                'model': model_name,\n",
    "                'dataset_idx': dataset_idx,\n",
    "                'epoch': epoch,\n",
    "                'auc': auc_seq[epoch],\n",
    "                'loss': loss_seq[epoch]\n",
    "            })\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(sequence, window_size=50):\n",
    "    return np.convolve(sequence, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "for model_name in model_dict.keys():\n",
    "    print(f\"\\nProcessing model {model_name}\")\n",
    "    \n",
    "    auc_evolutions = model_dict[model_name]['auc_evolution']\n",
    "    loss_evolutions = model_dict[model_name]['loss_evolution']\n",
    "\n",
    "    # Start and end loss\n",
    "    model_dict[model_name]['median_loss'] = np.median(loss_evolutions, axis=0)\n",
    "    model_dict[model_name]['mean_loss'] = np.mean(loss_evolutions, axis=0)\n",
    "    model_dict[model_name]['start_loss'] = np.mean(np.median(loss_evolutions, axis=0)[:5])\n",
    "    model_dict[model_name]['end_loss'] = np.mean(np.median(loss_evolutions, axis=0)[-5:])\n",
    "    model_dict[model_name]['mean_auc'] = np.mean(auc_evolutions, axis=0)\n",
    "\n",
    "    # Filter valid sequences\n",
    "    valid_indices = []\n",
    "    for i, loss_seq in enumerate(loss_evolutions):\n",
    "        if loss_seq[-1] <= 0.95 * loss_seq[0]:\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    print(f\"Valid sequences: {len(valid_indices)}\")\n",
    "    if not valid_indices:  # Skip if no valid sequences\n",
    "        model_dict[model_name]['mean_correlations'] = 0\n",
    "        model_dict[model_name]['correlation_mean'] = 0\n",
    "        model_dict[model_name]['valids'] = 0\n",
    "        continue\n",
    "        \n",
    "    # Keep only valid sequences\n",
    "    filtered_auc = [moving_average(auc_evolutions[i]) for i in valid_indices]\n",
    "    filtered_loss = [moving_average(loss_evolutions[i]) for i in valid_indices]\n",
    "    \n",
    "    # Compute correlations\n",
    "    correlations_seq = [np.corrcoef(filtered_auc[i], filtered_loss[i])[0,1] for i in range(len(filtered_auc))]\n",
    "    mean_corr = np.nanmean(correlations_seq)\n",
    "    model_dict[model_name]['mean_correlations'] = mean_corr\n",
    "\n",
    "    # Correlation of the mean curves\n",
    "    avg_auc = np.mean(filtered_auc, axis=0)\n",
    "    avg_loss = np.mean(filtered_loss, axis=0)\n",
    "    corr = np.corrcoef(avg_auc, avg_loss)[0,1]\n",
    "    model_dict[model_name]['correlation_mean'] = corr\n",
    "    \n",
    "    # Number of valid datasets\n",
    "    model_dict[model_name]['valids'] = len(valid_indices)\n",
    "\n",
    "print('')\n",
    "\n",
    "# Summary\n",
    "for model_name in model_dict.keys():\n",
    "    print(f\"{model_name:<10}: \"\n",
    "          f\"AUC: {model_dict[model_name]['auc']:.3f} - \"\n",
    "          f\"Valid Datasets: {model_dict[model_name]['valids']:<3} - \"\n",
    "          f\"Mean Correlation: {model_dict[model_name]['mean_correlations']:<6.3f} - \"\n",
    "          f\"Start Loss: {model_dict[model_name]['start_loss']:<10.3f} - \"\n",
    "          f\"End Loss: {model_dict[model_name]['end_loss']:<6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # Create a figure with 1 row and 2 columns\n",
    "\n",
    "# Plot for 'median_loss'\n",
    "variable = 'median_loss'\n",
    "(xmin, xmax) = (0, 300)  # X-axis limits\n",
    "(ymin, ymax) = (0, 80)  # Y-axis limits\n",
    "x_pos = 50  # Position at the end of the curve\n",
    "\n",
    "y_positions = [model_dict[model_name][variable][x_pos] for model_name in model_dict.keys()]\n",
    "ranking = 8 * np.argsort(np.argsort(y_positions)[::-1])\n",
    "x_offset = 60\n",
    "offsets = ranking - np.mean(ranking)\n",
    "\n",
    "it = 0\n",
    "for model_name in model_dict.keys():\n",
    "    median_loss = model_dict[model_name][variable]\n",
    "    axes[0].plot(median_loss, label=model_name, linewidth=2)\n",
    "\n",
    "    offset = offsets[it]\n",
    "    it += 1\n",
    "    y_pos = median_loss[x_pos]\n",
    "\n",
    "    if (y_pos - offset) < ymin:\n",
    "        offset = 0\n",
    "    if offset > y_pos:\n",
    "        offset = 0\n",
    "\n",
    "    axes[0].annotate(model_name.replace(\"_\", \"\"), xy=(x_pos, y_pos), xytext=(x_pos + x_offset, y_pos - offset),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='-'),\n",
    "                     fontsize=12)\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=16)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "axes[0].set_xlim(xmin, xmax)\n",
    "axes[0].set_ylim(ymin, ymax)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.25)\n",
    "axes[0].set_title(\"Median Loss Evolution\", fontsize=16)\n",
    "axes[0].tick_params(axis='both', labelsize=14)  # Change tick label size\n",
    "\n",
    "# Plot for 'mean_auc'\n",
    "variable = 'mean_auc'\n",
    "(xmin, xmax) = (0, 15)  # X-axis limits\n",
    "(ymin, ymax) = (0.85, 1.0)  # Y-axis limits\n",
    "x_pos = 2  # Position at the end of the curve\n",
    "\n",
    "y_positions = [model_dict[model_name][variable][x_pos] for model_name in model_dict.keys()]\n",
    "ranking = 0.01 * np.argsort(np.argsort(y_positions)[::-1])\n",
    "x_offset = 4\n",
    "offsets = ranking - np.mean(ranking)\n",
    "\n",
    "it = 0\n",
    "for model_name in model_dict.keys():\n",
    "    mean_auc = model_dict[model_name][variable]\n",
    "    axes[1].plot(mean_auc, label=model_name, linewidth=2)\n",
    "\n",
    "    offset = offsets[it]\n",
    "    it += 1\n",
    "    y_pos = mean_auc[x_pos]\n",
    "\n",
    "    if (y_pos - offset) < ymin:\n",
    "        offset = 0\n",
    "\n",
    "    axes[1].annotate(model_name.replace(\"_\", \"\"), xy=(x_pos, y_pos), xytext=(x_pos + x_offset, y_pos - offset),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='-'),\n",
    "                     fontsize=12)\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=16)\n",
    "axes[1].set_ylabel(\"AUC\", fontsize=16)\n",
    "axes[1].set_xlim(xmin, xmax)\n",
    "axes[1].set_ylim(ymin, ymax)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.25)\n",
    "axes[1].set_title(\"Mean AUC Evolution\", fontsize=16)\n",
    "axes[1].set_xticks([0, 5, 10, 15])\n",
    "axes[1].tick_params(axis='both', labelsize=14)  # Change tick label size\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root_dir, \"outputs/figs/training_Geological.pdf\"), format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Iterate through all model names and add their median losses to the plot\n",
    "for model_name in model_names:\n",
    "    mean_auc = model_dict[model_name]['mean_auc']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=mean_auc,\n",
    "        mode='lines',\n",
    "        name=model_name,\n",
    "        line=dict(width=2),\n",
    "        # hovertemplate=f\"{model_name}<br>Epoch: {{x}}<br>Loss: {{y:.2f}}<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Mean AUC Evolution for All Models\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Mean AUC\",\n",
    "    template=\"plotly_white\",\n",
    "    legend_title=\"Model Names\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    # yaxis_range=[0, 100],\n",
    "    xaxis_range=[0,50]\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['AE']['trial_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['GCN2MLP']['median_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# First filter the DataFrame\n",
    "df_ae = df.query('model == \"GCN2MLP\"')\n",
    "\n",
    "# Initialize figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the *initial* traces (first dataset_idx = 0 for example)\n",
    "initial_df = df_ae.query('dataset_idx == 0')\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=initial_df['epoch'],\n",
    "    y=moving_average(initial_df['loss']),\n",
    "    name='Loss',\n",
    "    yaxis='y1'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=initial_df['epoch'],\n",
    "    y=moving_average(initial_df['auc']),\n",
    "    name='AUC',\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "# Create frames — one for each dataset_idx\n",
    "frames = []\n",
    "for dataset_idx in sorted(df_ae['dataset_idx'].unique()):\n",
    "    frame_df = df_ae.query(f'dataset_idx == {dataset_idx}')\n",
    "    frames.append(go.Frame(\n",
    "        name=str(dataset_idx),\n",
    "        data=[\n",
    "            go.Scatter(x=frame_df['epoch'], y=moving_average(frame_df['loss'])),\n",
    "            go.Scatter(x=frame_df['epoch'], y=moving_average(frame_df['auc']))\n",
    "        ]\n",
    "    ))\n",
    "\n",
    "fig.frames = frames\n",
    "\n",
    "# Set up axes\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Epoch'),\n",
    "    yaxis=dict(\n",
    "        title='Loss',\n",
    "        titlefont=dict(color='#1f77b4'),\n",
    "        tickfont=dict(color='#1f77b4')\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='AUC',\n",
    "        titlefont=dict(color='#ff7f0e'),\n",
    "        tickfont=dict(color='#ff7f0e'),\n",
    "        anchor='x',\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    title='Loss and AUC vs Epoch (Animated by Dataset)',\n",
    "    legend=dict(#x=0.5, y=1.1,\n",
    "                orientation='v'),\n",
    "    \n",
    "    # Animation controls\n",
    "    updatemenus=[dict(\n",
    "        type=\"buttons\",\n",
    "        showactive=False,\n",
    "        # buttons=[dict(\n",
    "        #     label=\"Play\",\n",
    "        #     method=\"animate\",\n",
    "        #     args=[None, {\"frame\": {\"duration\": 1, \"redraw\": True},\n",
    "        #                  \"fromcurrent\": True}]\n",
    "        # )]\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Slider to manually control frames\n",
    "fig.update_layout(\n",
    "    sliders=[{\n",
    "        \"steps\": [{\n",
    "            \"args\": [[str(dataset_idx)], \n",
    "                     {\"frame\": {\"duration\": 1, \"redraw\": True},\n",
    "                      \"mode\": \"immediate\"}],\n",
    "            \"label\": f\"Dataset {dataset_idx}\",\n",
    "            \"method\": \"animate\",\n",
    "        } for dataset_idx in sorted(df_ae['dataset_idx'].unique())],\n",
    "        \"transition\": {\"duration\": 0},\n",
    "        \"x\": 0,\n",
    "        \"y\": -0.2,\n",
    "        \"currentvalue\": {\"prefix\": \"Dataset: \"}\n",
    "    }],\n",
    "    yaxis=dict(\n",
    "        title='Loss',\n",
    "        titlefont=dict(color='#1f77b4'),\n",
    "        tickfont=dict(color='#1f77b4'),\n",
    "        range=[0, 100]   # Set the range for Loss axis (left y-axis)\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='AUC',\n",
    "        titlefont=dict(color='#ff7f0e'),\n",
    "        tickfont=dict(color='#ff7f0e'),\n",
    "        anchor='x',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        range=[0.5, 1.1]  # Set the range for AUC axis (right y-axis)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# px.line(df.query('model==\"AE\"'), x='epoch', y=['loss','auc'], animation_frame='dataset_idx',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = 'GCN2MLP'\n",
    "\n",
    "loss_evolutions_ae = model_dict[model_name]['auc_evolution']\n",
    "\n",
    "# Plot all loss evolutions with low opacity\n",
    "plt.figure(figsize=(10, 6))\n",
    "for loss_seq in loss_evolutions_ae:\n",
    "    plt.plot(loss_seq, alpha=0.2, color='blue')\n",
    "\n",
    "plt.title(f'Loss Evolutions for Model {model_name}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlim(0, 300)\n",
    "# plt.ylim(0, 10)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN Initialization_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Geological_anomaly'\n",
    "initialization_metrics = torch.load(root_dir + f'/outputs/Optuna_analysis/initialization_{dataset_name}.pkl')\n",
    "\n",
    "optuna_metrics = {}\n",
    "for model in model_names:\n",
    "\n",
    "    auc_list = initialization_metrics[model]['auc']\n",
    "    f1_list = initialization_metrics[model]['f1']\n",
    "    mcc_list = initialization_metrics[model]['mcc']\n",
    "\n",
    "    optuna_metrics[model] = {\n",
    "        'mean_auc': np.mean(np.mean(auc_list,axis=0)).round(3),\n",
    "        'std_auc': np.std(np.mean(auc_list,axis=0)).round(3),\n",
    "        'mean_f1': np.mean(np.mean(f1_list,axis=0)).round(3),\n",
    "        'std_f1': np.std(np.mean(f1_list,axis=0)).round(3),\n",
    "        'mean_mcc': np.mean(np.mean(mcc_list,axis=0)).round(3),\n",
    "        'std_mcc': np.std(np.mean(mcc_list,axis=0)).round(3),\n",
    "    }\n",
    "\n",
    "# Create a nicely formatted table showing test metrics for all models\n",
    "metrics_df = pd.DataFrame()\n",
    "for model in optuna_metrics.keys():\n",
    "    metrics_df.loc[model, 'AUC'] = f\"{optuna_metrics[model]['mean_auc']:.3f} ± {optuna_metrics[model]['std_auc']:.3f}\"\n",
    "    metrics_df.loc[model, 'F1'] = f\"{optuna_metrics[model]['mean_f1']:.3f} ± {optuna_metrics[model]['std_f1']:.3f}\"\n",
    "    metrics_df.loc[model, 'MCC'] = f\"{optuna_metrics[model]['mean_mcc']:.3f} ± {optuna_metrics[model]['std_mcc']:.3f}\"\n",
    "\n",
    "print(metrics_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'EGMS_anomaly'\n",
    "initialization_metrics = torch.load(root_dir + f'/outputs/Optuna_analysis/initialization_{dataset_name}.pkl')\n",
    "\n",
    "optuna_metrics = {}\n",
    "for model in model_names:\n",
    "\n",
    "    auc_list = initialization_metrics[model]['auc']\n",
    "    f1_list = initialization_metrics[model]['f1']\n",
    "    mcc_list = initialization_metrics[model]['mcc']\n",
    "\n",
    "    optuna_metrics[model] = {\n",
    "        'mean_auc': np.mean(np.mean(auc_list,axis=0)).round(3),\n",
    "        'std_auc': np.std(np.mean(auc_list,axis=0)).round(3),\n",
    "        'mean_f1': np.mean(np.mean(f1_list,axis=0)).round(3),\n",
    "        'std_f1': np.std(np.mean(f1_list,axis=0)).round(3),\n",
    "        'mean_mcc': np.mean(np.mean(mcc_list,axis=0)).round(3),\n",
    "        'std_mcc': np.std(np.mean(mcc_list,axis=0)).round(3),\n",
    "    }\n",
    "\n",
    "# Create a nicely formatted table showing test metrics for all models\n",
    "metrics_df = pd.DataFrame()\n",
    "for model in optuna_metrics.keys():\n",
    "    metrics_df.loc[model, 'AUC'] = f\"{optuna_metrics[model]['mean_auc']:.3f} ± {optuna_metrics[model]['std_auc']:.3f}\"\n",
    "    metrics_df.loc[model, 'F1'] = f\"{optuna_metrics[model]['mean_f1']:.3f} ± {optuna_metrics[model]['std_f1']:.3f}\"\n",
    "    metrics_df.loc[model, 'MCC'] = f\"{optuna_metrics[model]['mean_mcc']:.3f} ± {optuna_metrics[model]['std_mcc']:.3f}\"\n",
    "\n",
    "print(metrics_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Geological_anomaly'\n",
    "initialization_metrics = torch.load(root_dir + f'/outputs/Optuna_analysis/initialization_{dataset_name}.pkl')\n",
    "\n",
    "model_names = ['AE', 'GCN2MLP', 'GCNAE', 'GConv2MLP', 'GConvAE', 'GUNet', 'RAE_GRU', 'RAE_LSTM']\n",
    "\n",
    "for model in model_names:\n",
    "    # STD Oof auc_values\n",
    "    if model in auc_dataset.keys():\n",
    "        print(f\"{model:<10}: {np.mean(np.mean(auc_dataset[model], axis=0)):.3f}  - {np.std(np.mean(auc_dataset[model], axis=0)):.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'EGMS_anomaly'\n",
    "auc_dataset = torch.load(root_dir + f'/outputs/Optuna_analysis/initialization_{dataset_name}.pkl')\n",
    "\n",
    "model_names = ['AE', 'GCN2MLP', 'GCNAE', 'GConv2MLP', 'GConvAE', 'GUNet', 'RAE_GRU', 'RAE_LSTM']\n",
    "\n",
    "for model in model_names:\n",
    "    # STD Oof auc_values\n",
    "    if model in auc_dataset.keys():\n",
    "        print(f\"{model:<10}: {np.mean(np.mean(auc_dataset[model], axis=0)):.3f}  - {np.std(np.mean(auc_dataset[model], axis=0)):.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pixel_mse(output,X):\n",
    "    point_mse = torch.nn.MSELoss(reduction='none')\n",
    "    return torch.mean(point_mse(output,X),axis=1)\n",
    "\n",
    "def evaluate_epochs(model, X, lr, n_epochs, label, edge_index=None, edge_weight=None, batch_size=None, rng_seed=0):\n",
    "    \n",
    "    torch.manual_seed(rng_seed)\n",
    "    torch.cuda.manual_seed(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    model.reset_parameters()\n",
    "\n",
    "    loss_evolution = []\n",
    "    auc_evolution = []\n",
    "\n",
    "    if isinstance(model, models.RAE):\n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X.T.unsqueeze(0)).squeeze(0).T\n",
    "            loss = criterion(output, X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            scores = pixel_mse(output, X).detach().cpu().numpy()\n",
    "            auc = get_auc(scores, label, resolution=101).round(3)\n",
    "\n",
    "            loss_evolution.append(loss.item())\n",
    "            auc_evolution.append(auc)\n",
    "\n",
    "    else:\n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(model, graph_models):\n",
    "                output = model(X, edge_index=edge_index, edge_weight=edge_weight)\n",
    "            else:\n",
    "                output = model(X)\n",
    "            loss = criterion(output, X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            scores = pixel_mse(output, X).detach().cpu().numpy()\n",
    "            # auc = get_auc(scores, label, resolution=101).round(3)\n",
    "\n",
    "            # loss_evolution.append(loss.item())\n",
    "            # auc_evolution.append(auc)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "graph_models = tuple([models.GCN2MLP, models.GConv2MLP, models.GCNAE, models.GConvAE, models.GUNet])\n",
    "\n",
    "dataset_name = 'EGMS_anomaly'\n",
    "datasets = torch.load(dataset_path + f'{dataset_name}/Training/dataset.pt')\n",
    "model_dict = torch.load(root_dir + f'/outputs/Optuna_analysis/model_dict_{dataset_name}.pkl')\n",
    "\n",
    "model_names = ['AE', 'GCN2MLP', 'GCNAE', 'GConv2MLP', 'GConvAE', 'GUNet', 'RAE_GRU', 'RAE_LSTM']\n",
    "\n",
    "auc_results = {}\n",
    "\n",
    "for model_name in model_names:  \n",
    "    # Iterate through datasets\n",
    "    loss_dataset = []\n",
    "    auc_dataset = []\n",
    "    for idx, dataset in enumerate(datasets, start=1):\n",
    "        print(f\"\\rProcessing dataset {idx}/{len(datasets)} for model {model_name}\", end=\"\", flush=True)\n",
    "\n",
    "        data = dataset['data']\n",
    "        label = dataset['label'].any(axis=1)\n",
    "\n",
    "        X = torch.tensor(data).float().to(device)\n",
    "\n",
    "        auc_seed = []\n",
    "        for seed in range(10):\n",
    "\n",
    "            model = copy.deepcopy(model_dict[model_name]['model']).to(device)\n",
    "\n",
    "            edge_index, edge_weight = (None, None)\n",
    "            if isinstance(model, graph_models):\n",
    "                edge_index = dataset['edge_index'].to(next(model.parameters()).device)\n",
    "                edge_weight = dataset['edge_weight'].to(next(model.parameters()).device)\n",
    "            if isinstance(model, models.RAE) and (model.n_features != 1):\n",
    "                relevant_params = ['n_features', 'latent_dim', 'rnn_type', 'rnn_act', 'device']\n",
    "                new_model_params = {key: getattr(model, key) for key in relevant_params}\n",
    "                new_model_params['n_features'] = X.shape[0]\n",
    "                new_model_params['device'] = device\n",
    "                model = models.RAE(**new_model_params)\n",
    "                model.to(new_model_params['device'])\n",
    "\n",
    "            scores = evaluate_epochs(model, X,\n",
    "                                        lr=model_dict[model_name]['trial_params']['lr'],\n",
    "                                        n_epochs=model_dict[model_name]['trial_params']['n_epochs'],\n",
    "                                        label=label,\n",
    "                                        edge_index=edge_index,\n",
    "                                        edge_weight=edge_weight,\n",
    "                                        rng_seed=seed)\n",
    "\n",
    "            auc = get_auc(scores, label, resolution=101).round(3)\n",
    "            auc_seed.append(auc)\n",
    "\n",
    "        auc_dataset.append(auc_seed)\n",
    "    \n",
    "    auc_results[model_name] = auc_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(np.mean(auc_dataset,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(auc_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(auc_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(auc_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(auc_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display standard deviation of AUC lists for all models\n",
    "for model_name in model_dict.keys():\n",
    "    auc_std = np.std(model_dict[model_name]['auc_list'])\n",
    "    print(f\"{model_name:<10}: {auc_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = joblib.load(root_dir + f'/outputs/HP_training/EGMS_anomaly/TR_GCNAE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().query('value_auc > 0.995 * value_auc.max()').sort_values('value_auc', ascending=False).head(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsensors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
